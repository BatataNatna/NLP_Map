{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## les bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import pandas as pd\n",
    "import spacy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_new_fake_theonion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>DAYTON, OH—While greeting the crowd at a campa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Presidential candidate Donald Trump recently r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>And the RNC is going to pay for it. It probabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>VANDALIA, OH—Drawing criticism for what many c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>WASHINGTON—Her mind spinning as she poured ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>1705</td>\n",
       "      <td>WASHINGTON—Claiming it felt queasy just thinki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>1706</td>\n",
       "      <td>WALDPORT, OR—A team of anthropologists announc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>1707</td>\n",
       "      <td>Donald Trump has stated publicly multiple time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1708</th>\n",
       "      <td>1708</td>\n",
       "      <td>WASHINGTON—Election boards across the country ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709</th>\n",
       "      <td>1709</td>\n",
       "      <td>If elected president, Hillary Clinton will hav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1710 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                           articles\n",
       "0              0  DAYTON, OH—While greeting the crowd at a campa...\n",
       "1              1  Presidential candidate Donald Trump recently r...\n",
       "2              2  And the RNC is going to pay for it. It probabl...\n",
       "3              3  VANDALIA, OH—Drawing criticism for what many c...\n",
       "4              4  WASHINGTON—Her mind spinning as she poured ove...\n",
       "...          ...                                                ...\n",
       "1705        1705  WASHINGTON—Claiming it felt queasy just thinki...\n",
       "1706        1706  WALDPORT, OR—A team of anthropologists announc...\n",
       "1707        1707  Donald Trump has stated publicly multiple time...\n",
       "1708        1708  WASHINGTON—Election boards across the country ...\n",
       "1709        1709  If elected president, Hillary Clinton will hav...\n",
       "\n",
       "[1710 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rymkm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rymkm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rymkm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer => vectorizer => transformer en vecteurs numerique\n",
    "nltk.download('punkt')\n",
    "#Stopwords => mots à supprimer\n",
    "nltk.download('stopwords')\n",
    "#Lemmatizer => se remettre au radical\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nettoyage des données**\n",
    "reduire la dim enleve les prefix et suffixes => modele plus performant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction pour nettoyer, tokenizer, lemmatiser  et stemmer\n",
    "def process_text(text):\n",
    "    #convertir en minuscules\n",
    "    text = text.lower()\n",
    "    #Supprimer les caractères non alphabétiques\n",
    "    text=re.sub(r'[^a-zA-Z\\s]','',text)\n",
    "    #Enlever html\n",
    "    text=re.sub(r'\\d+','',text)\n",
    "    #supprimer la ponctuation\n",
    "    text=re.sub(r'[^\\w\\s]', '', text)\n",
    "    #Tokenisation avec NLTK\n",
    "    tokens= word_tokenize(text)\n",
    "    #supprimer les stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens=[word for word in tokens if word not in stop_words]\n",
    "    #lemmatisation avec spacy\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    lemmatized_tokens=[token.lemma_ for token in doc]\n",
    "    #stemming \n",
    "    stemmer = PorterStemmer()\n",
    "    stemmer_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
    "    #Rejoindre les tokens en une chaine\n",
    "    processed_text = ' '.join(stemmer_tokens)\n",
    "    return  processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dayton ohwhil greet crowd campaign ralli thursday former presid donald trump see kiss support burger accord sourc attend well who s juici littl guy ask gop presidenti candid reportedli lift fulli load flamebroil beef hamburg hand press lip sesam seed bun emit audibl muah ounc wish could vote you re ador littl tasti burger be not be not preciou could take big bite right remind sandwich back home sourc confirm trump handler eventu interven would releas burger grip let ralli attende finish eat'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processed_text'] = df['articles'].apply(process_text)\n",
    "df['processed_text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "x_tokenized= [sentence.split() for sentence in df['processed_text']]\n",
    "model = gensim.models.Word2Vec(x_tokenized,min_count=100,window=5,vector_size=100)\n",
    "len(model.wv[\"effort\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
